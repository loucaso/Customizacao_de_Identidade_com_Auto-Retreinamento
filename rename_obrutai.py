import torch
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer

# ğŸ—ï¸ Caminho do modelo base
MODEL_PATH = "C:/ObrutAi"

# ğŸ“Œ Carregar dataset de identidade (apenas para mudanÃ§a de nome/resposta)
dataset = load_dataset("json", data_files="identity_override.jsonl", split="train")

# ğŸš€ Carregar tokenizador e modelo do caminho local
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, attn_implementation="eager")

# ğŸ”¥ Ativar RoPE Scaling para 128K com janela de 8K
model.config.update({
    "rope_scaling": {"type": "linear", "factor": 4},  # 32K * 4 = 128K
    "max_position_embeddings": 131072  # opcional, forÃ§a contexto mÃ¡ximo
})

# ğŸ› ï¸ FunÃ§Ã£o de tokenizaÃ§Ã£o com max_length 1024 (janela de atenÃ§Ã£o)
def tokenize(example):
    result = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=1024,
        return_tensors="pt"
    )
    return {
        "input_ids": result["input_ids"][0],
        "attention_mask": result["attention_mask"][0],
        "labels": result["input_ids"][0]
    }

# ğŸ” Tokenizar dataset de identidade
tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)

# ğŸ¯ ConfiguraÃ§Ã£o do primeiro treino (mudar identidade)
training_args = TrainingArguments(
    output_dir="D:/obrutai-identity-override",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    save_steps=10,
    save_total_limit=2,
    logging_steps=5,
    learning_rate=2e-5,
    weight_decay=0.01,
    fp16=True,
    bf16=False
)

# ğŸ”¥ Treinamento inicial
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

print("ğŸš€ Iniciando fine-tuning da identidade...")
trainer.train()

# ğŸ”„ Auto-retreinamento: o modelo aprende com suas prÃ³prias respostas
print("ğŸ”„ Iniciando auto-retreinamento supervisionado...")

auto_train_dataset = []
for _ in range(100):  # Gera 100 exemplos
    prompt = "Explique um conceito avanÃ§ado de programaÃ§Ã£o web: "
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)

    with torch.no_grad():
        outputs = model.generate(input_ids, max_new_tokens=1024)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    auto_train_dataset.append({"text": response})

# ğŸ“Œ Criar dataset sintÃ©tico e tokenizar
auto_train_dataset = Dataset.from_list(auto_train_dataset)
tokenized_auto_dataset = auto_train_dataset.map(tokenize, remove_columns=["text"])

# âš™ï¸ ConfiguraÃ§Ã£o do auto-retreinamento
auto_training_args = TrainingArguments(
    output_dir="D:/obrutai-self-trained",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=20,
    save_total_limit=2,
    logging_steps=10,
    learning_rate=1e-5,
    weight_decay=0.01,
    fp16=True,
    bf16=False
)

auto_trainer = SFTTrainer(
    model=model,
    args=auto_training_args,
    train_dataset=tokenized_auto_dataset,
)

print("ğŸ”¥ Treinando modelo com suas prÃ³prias previsÃµes...")
auto_trainer.train()

# ğŸ’¾ Salvar modelo final em D:
model.save_pretrained("D:/ObrutAi-tuned")
tokenizer.save_pretrained("D:/ObrutAi-tuned")

print("âœ… Treinamento finalizado! Modelo salvo em D:/ObrutAi-tuned")
